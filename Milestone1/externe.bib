
@book{brandes_physiologie_2019,
	address = {Berlin, Heidelberg},
	series = {Springer-{Lehrbuch}},
	title = {Physiologie des {Menschen}: mit {Pathophysiologie}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-662-56467-7 978-3-662-56468-4},
	shorttitle = {Physiologie des {Menschen}},
	url = {http://link.springer.com/10.1007/978-3-662-56468-4},
	language = {de},
	urldate = {2025-04-25},
	publisher = {Springer Berlin Heidelberg},
	editor = {Brandes, Ralf and Lang, Florian and Schmidt, Robert F.},
	year = {2019},
	doi = {10.1007/978-3-662-56468-4},
}

@article{tatler_nystagmus_2003,
	title = {On {Nystagmus}, {Saccades}, and {Fixations}},
	volume = {32},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0301-0066, 1468-4233},
	url = {https://journals.sagepub.com/doi/10.1068/p3395},
	doi = {10.1068/p3395},
	abstract = {Investigations of the ways in which the eyes move came to prominence in the 19th century, but techniques for measuring them more precisely emerged in the 20th century. When scanning a scene or text the eyes engage in periods of relative stability (fixations) interspersed with ballistic rotations (saccades). The saccade-and-fixate strategy, associated with voluntary eye movements, was first uncovered in the context of involuntary eye movements following body rotation. This pattern of eye movements is now referred to as nystagmus, and involves periods of slow eye movements, during which objects are visible, and rapid returns, when they are not; it is based on a vestibular reflex which attempts to achieve image stabilisation. Post-rotational nystagmus was reported in the late 18th century (by Wells), with afterimages used as a means of retinal stabilisation to distinguish between movement of the eyes and of the environment. Nystagmus was linked to vestibular stimulation in the 19th century, and Mach, Breuer, and Crum Brown all described its fast and slow phases. Wells and Breuer proposed that there was no visual awareness during the ballistic phase (saccadic suppression). The saccade-and-fixate strategy highlighted by studies of nystagmus was shown to apply to tasks like reading by Dodge, who used more sophisticated photographic techniques to examine oculomotor kinematics. The relationship between eye movements and perception, following earlier intuitions by Wells and Breuer, was explored by Dodge, and has been of fundamental importance in the direction of vision research over the last century.},
	language = {en},
	number = {2},
	urldate = {2025-04-25},
	journal = {Perception},
	author = {Tatler, Benjamin W and Wade, Nicholas J},
	month = feb,
	year = {2003},
	pages = {167--184},
}

@book{gilchrist_saccades_2011,
	title = {Saccades},
	url = {https://academic.oup.com/edited-volume/41257/chapter/350822882},
	urldate = {2025-04-25},
	publisher = {Oxford University Press},
	author = {Gilchrist, Iain},
	month = aug,
	year = {2011},
	doi = {10.1093/oxfordhb/9780199539789.013.0005},
}

@book{larman_uml_2009,
	address = {Frechen},
	edition = {1. Aufl., [Nachdr.]},
	series = {Software-{Entwicklung}},
	title = {{UML} 2 und {Patterns} angewendet - objektorientierte {Softwareentwicklung}: use cases, "{Gang} of {Four}"-{Patterns} und {GRASP}; umfangreiche {Fallstudien} und zahlreiche {Praxistipps}},
	isbn = {978-3-8266-1453-8},
	shorttitle = {{UML} 2 und {Patterns} angewendet - objektorientierte {Softwareentwicklung}},
	publisher = {Mitp},
	author = {Larman, Craig},
	year = {2009},
}

@article{olsen_tobii_2012,
	title = {The {Tobii} {I}-{VT} {Fixation} {Filter}},
	copyright = {Tobii Technology},
	author = {Olsen, Anneli},
	month = mar,
	year = {2012},
	pages = {21},
}

@article{andersson_one_2017,
	title = {One algorithm to rule them all? {An} evaluation and discussion of ten eye movement event-detection algorithms},
	volume = {49},
	issn = {1554-3528},
	shorttitle = {One algorithm to rule them all?},
	url = {http://link.springer.com/10.3758/s13428-016-0738-9},
	doi = {10.3758/s13428-016-0738-9},
	number = {2},
	urldate = {2025-04-23},
	journal = {Behavior Research Methods},
	author = {Andersson, Richard and Larsson, Linnea and Holmqvist, Kenneth and Stridh, Martin and Nyström, Marcus},
	month = apr,
	year = {2017},
	pages = {616--637},
}

@misc{tobii_net_2021,
	title = {Net sales of {Tobii} from 2014 to 2020 (in million {SEK}) [{Graph}]},
	url = {https://www.statista.com/statistics/1003717/net-sales-tobii/},
	abstract = {In 2020, the net sales of Tobii, the worldwide leading company in eye tracking technology,amounted to almost 1.43 billion Swedish kronor.},
	urldate = {2025-04-23},
	journal = {Statista},
	author = {{Tobii}},
	month = feb,
	year = {2021},
}

@book{langr_modern_2013,
	address = {Raleigh},
	edition = {1st ed},
	title = {Modern {C}++ {Programming} with {Test}-{Driven} {Development}: {Code} {Better}, {Sleep} {Better}},
	isbn = {978-1-68050-402-6},
	shorttitle = {Modern {C}++ {Programming} with {Test}-{Driven} {Development}},
	abstract = {Cover -- Table of Contents -- Foreword -- Introduction -- But Can It Work for Me on My System? -- Who This Book Is For -- What You'll Need -- How to Use This Book -- About "Us" -- About Me -- About the C++ Style in This Book -- Acknowledgments -- Dedication -- 1. Global Setup -- Setup -- The Examples -- C++ Compiler -- CMake -- Google Mock -- CppUTest -- libcurl -- JsonCpp -- rlog -- Boost -- Building Examples and Running Tests -- Teardown -- 2. Test-Driven Development: A First Example -- Setup -- The Soundex Class -- Getting Started -- Fixing Unclean Code -- Incrementalism -- Fixtures and Setup -- Thinking and TDD -- Test-Driving vs. Testing -- What If? -- One Thing at a Time -- Limiting Length -- Dropping Vowels -- Doing What It Takes to Clarify Tests -- Testing Outside the Box -- Back on Track -- Refactoring to Single-Responsibility Functions -- Finishing Up -- What Tests Are We Missing? -- Our Solution -- The Soundex Class -- Teardown -- 3. Test-Driven Development Foundations -- Setup -- Unit Test and TDD Fundamentals -- The TDD Cycle: Red-Green-Refactor -- The Three Rules of TDD -- Getting Green on Red -- Mind-Sets for Successful Adoption of TDD -- Mechanics for Success -- Teardown -- 4. Test Construction -- Setup -- Organization -- Fast Tests, Slow Tests, Filters, and Suites -- Assertions -- Inspecting Privates -- Testing vs. Test-Driving: Parameterized Tests and Other Toys -- Teardown -- 5. Test Doubles -- Setup -- Dependency Challenges -- Test Doubles -- A Hand-Crafted Test Double -- Improving Test Abstraction When Using Test Doubles -- Using Mock Tools -- Getting Test Doubles in Place -- Design Will Change -- Strategies for Using Test Doubles -- Miscellaneous Test Double Topics -- Teardown -- 6. Incremental Design -- Setup -- Simple Design -- Where Is the Up-Front Design? -- Refactoring Inhibitors -- Teardown -- 7. Quality Tests -- Setup},
	publisher = {Pragmatic Programmers, LLC, The},
	author = {Langr, Jeff},
	year = {2013},
}

@inproceedings{olsen_identifying_2012,
	address = {Santa Barbara California},
	title = {Identifying parameter values for an {I}-{VT} fixation filter suitable for handling data sampled with various sampling frequencies},
	isbn = {978-1-4503-1221-9},
	url = {https://dl.acm.org/doi/10.1145/2168556.2168625},
	doi = {10.1145/2168556.2168625},
	urldate = {2025-04-23},
	booktitle = {Proceedings of the {Symposium} on {Eye} {Tracking} {Research} and {Applications}},
	publisher = {ACM},
	author = {Olsen, Anneli and Matos, Ricardo},
	month = mar,
	year = {2012},
	pages = {317--320},
}

@inproceedings{salvucci_identifying_2000,
	address = {Palm Beach Gardens, Florida, United States},
	title = {Identifying fixations and saccades in eye-tracking protocols},
	isbn = {978-1-58113-280-9},
	url = {http://portal.acm.org/citation.cfm?doid=355017.355028},
	doi = {10.1145/355017.355028},
	urldate = {2025-04-23},
	booktitle = {Proceedings of the symposium on {Eye} tracking research \& applications  - {ETRA} '00},
	publisher = {ACM Press},
	author = {Salvucci, Dario D. and Goldberg, Joseph H.},
	year = {2000},
	pages = {71--78},
}

@book{stuart_eye_2022,
	address = {New York, NY},
	series = {Neuromethods},
	title = {Eye {Tracking}: {Background}, {Methods}, and {Applications}},
	volume = {183},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-1-0716-2390-9 978-1-0716-2391-6},
	shorttitle = {Eye {Tracking}},
	url = {https://link.springer.com/10.1007/978-1-0716-2391-6},
	urldate = {2025-04-23},
	publisher = {Springer US},
	editor = {Stuart, Samuel},
	year = {2022},
	doi = {10.1007/978-1-0716-2391-6},
}

@misc{noauthor_eye_nodate,
	title = {Eye {Tracking} {Market} {Share}},
	copyright = {Global Market Insights Inc.},
	url = {https://www.gminsights.com/industry-analysis/eye-tracking-market},
	abstract = {The eye tracking market size crossed USD 850 million in 2023 and is projected to expand at around 30.5\% CAGR from 2024 to 2032, driven by the growing demand in eye tracking for automotive applications.},
	urldate = {2025-04-23},
	journal = {Global Market Insights Inc.},
}

@misc{tobii_ab_eye_2022,
	title = {Eye movement classification},
	copyright = {Tobii AB},
	url = {https://connect.tobii.com/s/article/eye-movement-classification?language=en_US},
	abstract = {Get in touch with Tobii Customer Care team!},
	urldate = {2025-04-23},
	author = {{Tobii AB}},
	month = nov,
	year = {2022},
}

@incollection{winkler_effects_2014,
	address = {Cham},
	title = {Effects of {Test}-{Driven} {Development}: {A} {Comparative} {Analysis} of {Empirical} {Studies}},
	volume = {166},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-03601-4},
	shorttitle = {Effects of {Test}-{Driven} {Development}},
	url = {http://link.springer.com/10.1007/978-3-319-03602-1_10},
	urldate = {2025-04-23},
	booktitle = {Software {Quality}. {Model}-{Based} {Approaches} for {Advanced} {Software} and {Systems} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Mäkinen, Simo and Münch, Jürgen},
	editor = {Winkler, Dietmar and Biffl, Stefan and Bergsmann, Johannes},
	year = {2014},
	doi = {10.1007/978-3-319-03602-1_10},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {155--169},
}

@misc{martin_konopka_gazetoolkit_2019,
	title = {{GazeToolkit} [{GitHub} repository]},
	copyright = {BSD-3-Clause},
	url = {https://github.com/uxifiit/GazeToolkit},
	abstract = {Toolkit for processing eye movement data, fixation filtering, smoothing, etc.},
	urldate = {2025-04-23},
	author = {{Martin Konopka}},
	year = {2019},
	keywords = {c-sharp, eyetracking, filter, fixation, fixation-filtering, gaze, tobii},
}

@misc{krafka_eye_2016,
	title = {Eye {Tracking} for {Everyone}},
	url = {http://arxiv.org/abs/1606.05814},
	doi = {10.48550/arXiv.1606.05814},
	abstract = {From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.},
	urldate = {2025-04-23},
	publisher = {arXiv},
	author = {Krafka, Kyle and Khosla, Aditya and Kellnhofer, Petr and Kannan, Harini and Bhandarkar, Suchendra and Matusik, Wojciech and Torralba, Antonio},
	month = jun,
	year = {2016},
	note = {arXiv:1606.05814 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ezer_hand-labeled_2025,
	title = {Hand-{Labeled} {Eye} {Movements} ({Fixations}, {Saccades}, {Post}-{Saccadic} {Oscillations}): {Tobii} {Pro} {Fusion} (250 {Hz}) and {Spectrum} (300 {Hz})},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Hand-{Labeled} {Eye} {Movements} ({Fixations}, {Saccades}, {Post}-{Saccadic} {Oscillations})},
	url = {https://zenodo.org/doi/10.5281/zenodo.15034922},
	doi = {10.5281/ZENODO.15034922},
	abstract = {Hand-labeled ground truth dataset used and described in:Timur Ezer, Fabian Engl, Lisa Grabinger, Simon Röhrl, Florian Hauser, and Jürgen Mottok. 2025. A Comparison of Eye Movement Classifiers. In ECSEE 2025: European Conference on Software Engineering Education (ECSEE 2025), June 02--04, 2025, Seeon, Germany. ACM, New York, NY, USA 10 Pages. https://doi.org/10.1145/3723010.3723023evt values have the following meanings:

evt = 0: Undefined eye movement

evt= 1: Fixation

evt=2: Saccade

evt=3: Post-saccadic oscillation},
	urldate = {2025-04-23},
	publisher = {Zenodo},
	author = {Ezer, Timur and Engl, Fabian},
	month = mar,
	year = {2025},
}

@article{hauser_visuelle_2024,
	title = {Visuelle {Expertise} bei {Code} {Reviews}},
	url = {https://epub.uni-regensburg.de/id/eprint/59753},
	doi = {10.5283/EPUB.59753},
	abstract = {The aim of this PhD thesis is to investigate visual expertise in code reviews using eye tracking. In its theoretical assumptions, it draws on the theories of expertise research as well as on the holistic models of image perception originating from radiology and applies these in the context of software engineering. The properties of visual expertise are investigated for both the procedural programming language C and the object-oriented programming language C++ in one study each. 
The C study primarily addresses the ability to find errors in source code. It is 
based on the research designs of earlier work by other researchers, but extends their approaches. The design is supplemented with new theoretical foundations, additional survey methods and in-depth analyses. It uses data from 23 subjects (n(novices)=15; n(experts)=8) whose eye movements are recorded by an SMI 250RED mobile eye tracker. The results suggest that there are experience-related differences between the experts and novices: Experts are able to find more errors with less visual effort. It is also apparent that the reviews are carried out in phases that are each characterized by dominant strategies. 
The C++ study uses a similar design and methodology , but adds correct examples as distractors. Its sample comprises 34 subjects (n(novices)=18; n(experts)=16). The eye movements in this study are recorded by a Tobii Pro Spectrum. The results also indicate that the experts and novices differ in their approach and that experience has an influence on the way how the review is conducted. It can also be confirmed that the review of the source codes takes place in phases, each of which is again characterized by a dominant strategy . 
Overall, the results of the two studies provide evidence that holistic models of image perception are suitable for analyzing and interpreting eye movements during a code review and can explain visual expertise in this domain. Finally , these models are combined and an impulse for future studies on this topic is given.},
	urldate = {2025-04-23},
	author = {Hauser, Florian},
	year = {2024},
	note = {Publisher: Universität Regensburg},
	keywords = {004 Informatik, 370 Erziehung, Schul- und Bildungswesen, Visuelle Expertise; Code Reviews; Expertiseforschung; Eye Tracking; Software Engineering; Holistic Models of Image Perception},
}

@article{trabulsi_optimizing_2021,
	title = {Optimizing {Fixation} {Filters} for {Eye}-{Tracking} on {Small} {Screens}},
	volume = {15},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2021.578439/full},
	doi = {10.3389/fnins.2021.578439},
	abstract = {The study of consumer responses to advertising has recently expanded to include the use of eye-tracking to track the gaze of consumers. The calibration and validation of eye-gaze have typically been measured on large screens in static, controlled settings. However, little is known about how precise gaze localizations and eye fixations are on smaller screens, such as smartphones, and in moving feed-based conditions, such as those found on social media websites. We tested the precision of eye-tracking fixation detection algorithms relative to raw gaze mapping in natural scrolling conditions. Our results demonstrate that default fixation detection algorithms normally employed by hardware providers exhibit suboptimal performance on mobile phones. In this paper, we provide a detailed account of how different parameters in eye-tracking software can affect the validity and reliability of critical metrics, such as Percent Seen and Total Fixation Duration. We provide recommendations for producing improved eye-tracking metrics for content on small screens, such as smartphones, and vertically moving environments, such as a social media feed. The adjustments to the fixation detection algorithm we propose improves the accuracy of Percent Seen by 19\% compared to a leading eye-tracking provider’s default fixation filter settings. The methodological approach provided in this paper could additionally serve as a framework for assessing the validity of applied neuroscience methods and metrics beyond mobile eye-tracking.},
	urldate = {2025-04-23},
	journal = {Frontiers in Neuroscience},
	author = {Trabulsi, Julia and Norouzi, Kian and Suurmets, Seidi and Storm, Mike and Ramsøy, Thomas Zoëga},
	month = nov,
	year = {2021},
	pages = {578439},
}

@misc{docker_inc_what_nodate,
	title = {What is {Docker}?},
	copyright = {Docker Inc},
	url = {https://docs.docker.com/get-started/docker-overview/},
	abstract = {Get an in-depth overview of the Docker platform including what it can be used for, the architecture it employs, and its underlying technology.},
	language = {en},
	urldate = {2025-04-23},
	journal = {Docker Documentation},
	author = {{Docker Inc}},
}
