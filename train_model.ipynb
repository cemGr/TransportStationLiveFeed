{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590031aab40005e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bike-demand forecasting – fast RandomForest tune on Apple M-series\n",
    "=================================================================\n",
    "• RandomForest *only*  – no model change\n",
    "• Successive-halving search (~15% of the fits a full grid would need)\n",
    "• Uses all CPU cores once (outer CV n_jobs = –1, trees single-threaded)\n",
    "\n",
    "Requires: scikit-learn ≥ 1.4, pandas, numpy, joblib\n",
    "-----------------------------------------------------------------\n",
    "pip install -U scikit-learn pandas numpy joblib\n",
    "\"\"\"\n",
    "\n",
    "# ---------- imports & env -----------------------------------------------\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    HalvingRandomSearchCV,\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from joblib import dump, parallel_backend\n",
    "\n",
    "# ---------- config -------------------------------------------------------\n",
    "CSV_PATH      = \"trips_for_model.csv\"\n",
    "TARGET_COL    = \"bikes_taken\"\n",
    "TIMESTAMP_COL = \"slot_ts\"\n",
    "CORES         = os.cpu_count() or 8       # M4 Pro shows 12–14\n",
    "\n",
    "# keep each tree single-threaded → no nested OpenMP contention\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# ---------- 1. load ------------------------------------------------------\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "if TIMESTAMP_COL not in df.columns:\n",
    "    raise KeyError(f\"Column “{TIMESTAMP_COL}” not found.\")\n",
    "df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL])\n",
    "\n",
    "# ---------- 2. feature engineering --------------------------------------\n",
    "df[\"hour\"]       = df[TIMESTAMP_COL].dt.hour\n",
    "df[\"weekday\"]    = df[TIMESTAMP_COL].dt.dayofweek\n",
    "df[\"is_weekend\"] = df[\"weekday\"].isin([5, 6])\n",
    "\n",
    "cat_cols = [\"temp_class\", \"season\", \"hour\",\n",
    "            \"weekday\", \"cluster_id\", \"is_weekend\"]\n",
    "num_cols = (\n",
    "    df.select_dtypes(include=[\"number\", \"bool\"])\n",
    "      .columns.difference(cat_cols + [TARGET_COL, TIMESTAMP_COL])\n",
    "      .tolist()\n",
    ")\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    [(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "     (\"num\", \"passthrough\",                            num_cols)],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# ---------- 3. time-split train / val / test -----------------------------\n",
    "df = df.sort_values(TIMESTAMP_COL)\n",
    "n  = len(df)\n",
    "train_end = math.floor(n * 0.70)\n",
    "val_end   = math.floor(n * 0.85)\n",
    "\n",
    "train = df.iloc[:train_end]\n",
    "val   = df.iloc[train_end:val_end]\n",
    "test  = df.iloc[val_end:]\n",
    "\n",
    "X_train, y_train = train.drop(columns=[TARGET_COL]), train[TARGET_COL]\n",
    "X_val,   y_val   = val.drop(columns=[TARGET_COL]),   val[TARGET_COL]\n",
    "X_test,  y_test  = test.drop(columns=[TARGET_COL]),  test[TARGET_COL]\n",
    "\n",
    "# ---------- 4. quick baseline RF (200 trees) -----------------------------\n",
    "baseline = Pipeline([\n",
    "    (\"preproc\", preproc),\n",
    "    (\"rf\", RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,        # forest parallelism OK for *one* fit\n",
    "            verbose=2))\n",
    "])\n",
    "t0 = time.perf_counter()\n",
    "baseline.fit(X_train, y_train)\n",
    "print(f\"Baseline RF in {time.perf_counter()-t0:.1f}s – \"\n",
    "      f\"val MAE {mean_absolute_error(y_val, baseline.predict(X_val)):.3f}\")\n",
    "\n",
    "# ---------- 5. successive-halving hyper-search (RandomForest) -----------\n",
    "rf_search = RandomForestRegressor(\n",
    "        n_estimators=50,      # tiny resource for first round\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbose=2)            # <-- trees serial, CV parallel\n",
    "\n",
    "param_dist = {\n",
    "    \"rf__max_depth\":           [None, 8, 12, 16, 20],\n",
    "    \"rf__max_features\":        [\"sqrt\", 0.3, 0.5, 0.7],\n",
    "    \"rf__min_samples_leaf\":    [1, 2, 4],\n",
    "    \"rf__min_samples_split\":   [2, 5, 10],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([(\"preproc\", preproc),\n",
    "                 (\"rf\", rf_search)])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "    estimator           = pipe,\n",
    "    param_distributions = param_dist,\n",
    "    n_candidates        = 70,          # start ideas\n",
    "    factor              = 3,           # keep best 1/3 each iteration\n",
    "    resource            = \"rf__n_estimators\",\n",
    "    max_resources       = 300,         # final round = 300-tree forests\n",
    "    min_resources       = 50,\n",
    "    random_state        = 42,\n",
    "    cv                  = tscv,\n",
    "    scoring             = \"neg_mean_absolute_error\",\n",
    "    n_jobs              = CORES,       # one fit per core\n",
    "    verbose             = 3,\n",
    ")\n",
    "\n",
    "print(\"\\n⏳ Successive-halving search (RandomForest)…\")\n",
    "t0 = time.perf_counter()\n",
    "with parallel_backend(\"loky\"):\n",
    "    search.fit(pd.concat([X_train, X_val]),\n",
    "               pd.concat([y_train, y_val]))\n",
    "print(f\"Search finished in {time.perf_counter()-t0:.1f}s\")\n",
    "print(\"Best params:\", search.best_params_)\n",
    "\n",
    "# ---------- 6. refit best params on FULL train+val with 600 trees ---------\n",
    "best_rf_params = {k.split(\"__\", 1)[1]: v for k, v in search.best_params_.items()}\n",
    "best_rf_params.update(dict(n_estimators=600,\n",
    "                           random_state=42,\n",
    "                           n_jobs=CORES,\n",
    "                           verbose=2))   # final forest uses all cores\n",
    "\n",
    "final_model = Pipeline([\n",
    "    (\"preproc\", preproc),\n",
    "    (\"rf\", RandomForestRegressor(**best_rf_params))\n",
    "])\n",
    "\n",
    "print(\"\\n⏳ Final RandomForest (600 trees)…\")\n",
    "t0 = time.perf_counter()\n",
    "with parallel_backend(\"loky\"):\n",
    "    final_model.fit(pd.concat([X_train, X_val]),\n",
    "                    pd.concat([y_train, y_val]))\n",
    "print(f\"Final fit done in {time.perf_counter()-t0:.1f}s\")\n",
    "\n",
    "# ---------- 7. evaluation -------------------------------------------------\n",
    "y_pred = final_model.predict(X_test)\n",
    "print(f\"\\nTest MAE:  {mean_absolute_error(y_test, y_pred):.3f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}\")\n",
    "print(f\"Test R²:   {r2_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "feat_names = final_model.named_steps[\"preproc\"].get_feature_names_out()\n",
    "imps       = final_model.named_steps[\"rf\"].feature_importances_\n",
    "top        = np.argsort(imps)[::-1][:10]\n",
    "print(\"\\nTop-10 feature importances:\")\n",
    "for i in top:\n",
    "    print(f\"{feat_names[i]}: {imps[i]:.4f}\")\n",
    "\n",
    "# ---------- 8. save -------------------------------------------------------\n",
    "dump(final_model, \"rf_hourly.pkl\")\n",
    "print(\"\\n✅  Model saved to rf_hourly.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
